{"cells":[{"cell_type":"markdown","source":["## Commercial Data Developer - Interview Exercise\n\nThe Problem\nWe have a stream of event level data from our audience data platform (see sample below).\nWithin this are events associating anonymous customer ids with behavioral segment ids.\nFrom this stream we would like to build a customer-segment data set from which we can\nfind:\n- How many customers are in each segment\n- For an individual customer, which segments are they in\nThe Exercise"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"96816ed7-b08a-4ca3-abb9-f99880c3535d"}}},{"cell_type":"markdown","source":["I used apache spark with databrick platform to analyse and to answer above problem statements"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6b0f687-466a-4242-9839-9a5aae6aa2f5"}}},{"cell_type":"markdown","source":["Down loaded data from S3 location (https://reach-solutions-interview-data.s3-eu-west-1.amazonaws.com) placed at \"/FileStore/tables/wagal/\".\n\nLoaded data using spark and created dataframe for future anlysis"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8b423e52-4854-4354-bcc9-fc77ba23f2db"}}},{"cell_type":"code","source":["# File location and type\nfile_location = \"/FileStore/tables/wagal/*\"\nfile_type = \"json\"\n\n# CSV options\ninfer_schema = \"false\"\nfirst_row_is_header = \"false\"\ndelimiter = \",\"\n\n# The applied options are for CSV files. For other file types, these will be ignored.\ndf = spark.read.format(file_type) \\\n  .option(\"inferSchema\", infer_schema) \\\n  .option(\"header\", first_row_is_header) \\\n  .option(\"sep\", delimiter) \\\n  .load(file_location)\n\ndisplay(df)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6482be4c-f067-47c9-b0ac-35c938b94601"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["#Filter out the events for GB users only, which associate customer ids with 1st party behavioral segment ids"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b598a697-9df0-473b-9f40-df2bd17fd45b"}}},{"cell_type":"code","source":["#Diplay the schema to see available fields\ndf.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"97cef085-abfa-4cea-bc05-b7b49bdfb652"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql.functions import *\n\n#Flatten the events array and fill  1st party behavioral segment ids with -1 which are not associated with customer ids \nflatten_df = df.withColumn(\"event\", explode(\"events\")) \\\n.na.fill({'event.c': -1}) \\\n.na.fill({'event.add':  -1})\n\n\n#Filter recrods which are associate customer ids with 1st party behavioral segment ids\nres_df = flatten_df.filter((flatten_df.country == \"GB\") & (flatten_df.event.c != -1)  ).drop(\"events\")\n\n#Pritnt schema of dataframe.\nres_df.printSchema()\n\n#Show some sample records.\nres_df.show()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f7bcac2c-5686-42a9-a735-a3383be4d1f1"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["To generate the report using sql way we need to create a temp table in spark"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"ddd3a5ae-c32a-4e1b-bafb-99c2c0993009"}}},{"cell_type":"code","source":["temp_table_name = \"view_customer_segemnt\"\nres_df.createOrReplaceTempView(temp_table_name)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f15801cf-7dde-4a51-8d41-587901d9619d"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["# Question 1 : How many customers are in each segment"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"28e19365-bfca-4579-8820-33b03ee0df28"}}},{"cell_type":"code","source":["%sql  \n\nSELECT indi_segment_id as segment_id, count(id.val)as customer_count\nFROM view_customer_segemnt \nLATERAL VIEW explode(event.add) t_segment AS indi_segment_id\nGROUP BY indi_segment_id"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"0ecc1355-7fba-4dcf-ae8b-1de2dc339668"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Doing same as above, but using spark sql, however the query is same. \n\nThe advanatge is we can do any other operation on dataframe after we anlysys as like saving data in to a file etc"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1671b9c4-787d-40f9-ad77-ed371a4ef180"}}},{"cell_type":"code","source":["\nquery = \"SELECT indi_segment_id as segment_id, count(id.val) as customer_count FROM view_customer_segemnt LATERAL VIEW explode(event.add) t_segment AS indi_segment_id GROUP BY indi_segment_id\"\n\nres1_df = spark.sql(query)\n\nres1_df.write.mode(\"overwrite\").format(\"json\").json(\"/FileStore/tables/count_customers_segment\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"da41a307-f0c5-4a3d-9092-ef5903ce6909"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["##Questions 2: For an individual customer, which segments are they in"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"9a97fa23-9acc-4fe6-91b4-f0396adcc918"}}},{"cell_type":"code","source":["%sql\n\nSELECT id.val as customer_id, collect_set(indi_segment_id) as related_customer_segments_ids, count(indi_segment_id) count_related_customer_segments_ids\nFROM view_customer_segemnt \nLATERAL VIEW explode(event.add) t_segment AS indi_segment_id\nGROUP BY id;\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8390ea99-ccbc-41ea-b2bd-f838c953a930"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["# Doing same as above, but using spark sql, however the query is same. \n# The advanatge is we can do any other operation on dataframe after we anlysys as like saving data in to a file etc\nquery = \"\"\"SELECT id.val as customer_id, collect_set(indi_segment_id) as related_customer_segments_ids, count(indi_segment_id) count_related_customer_segments_ids\nFROM view_customer_segemnt \\\nLATERAL VIEW explode(event.add) t_segment AS indi_segment_id \\\nGROUP BY id\"\"\"\n\nres2_df = spark.sql(query)\n\nres2_df.show()\n\nres2_df.write.mode(\"overwrite\").format(\"json\").json(\"/FileStore/tables/customers_segment\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a383deae-d391-414b-8f08-2d7327875b8e"}},"outputs":[],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Wagal - Commercial Data Developer - Interview Exercise","dashboards":[{"elements":[{"elementNUID":"96816ed7-b08a-4ca3-abb9-f99880c3535d","guid":"93b5d889-9dfb-4544-8cdb-b3b28241193c","options":null,"position":{"x":0,"y":0,"height":8,"width":12,"z":null},"elementType":"command"}],"guid":"60c4b3fd-e9b1-4cdc-b95b-10fbdeb9aeaf","layoutOption":{"stack":true,"grid":true},"version":"DashboardViewV1","nuid":"0a88a002-637a-4ba9-bfbc-339dcb14630a","origId":21862810226038,"title":"Untitled","width":1024,"globalVars":{}}],"language":"python","widgets":{},"notebookOrigID":1951586186020076}},"nbformat":4,"nbformat_minor":0}
